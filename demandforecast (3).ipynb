{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91818f54-6dc4-4ae8-98d5-7cb22e6b308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demandforecast(parameters):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import tensorflow as tf \n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "    from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout\n",
    "    import keras\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logging.info(\"--------------------------------------------------------------------------------------\")\n",
    "    logging.info(f\"Input Parameters: {parameters}\")\n",
    "    logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    lr = float(parameters[\"lr\"])\n",
    "    num_epoch = int(parameters[\"num_epoch\"])\n",
    "\n",
    "    \n",
    "    is_dist = parameters[\"is_dist\"]\n",
    "    num_workers = parameters[\"num_workers\"]\n",
    "    batch_size_per_worker = 64\n",
    "    batch_size_global = batch_size_per_worker * num_workers\n",
    "   \n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            logging.info(\n",
    "                \"Epoch {}/{}. mse={:.4f} - loss={:.4f}\".format(\n",
    "                    epoch+1, num_epoch, logs[\"mse\"], logs[\"loss\"]\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    sales_path='https://raw.githubusercontent.com/KantSagar/SampleData/master/sales_train.csv'\n",
    "    items_path='https://raw.githubusercontent.com/KantSagar/SampleData/master/items.csv'\n",
    "    resturant_path='https://raw.githubusercontent.com/KantSagar/SampleData/master/resturants.csv'\n",
    "    \n",
    "    df_sales = pd.read_csv(sales_path)\n",
    "    df_items = pd.read_csv(items_path)\n",
    "    df_resturant = pd.read_csv(resturant_path)\n",
    "    \n",
    "    \n",
    "    def series_to_supervised(data, window=1, lag=1, dropnan=True):   \n",
    "        cols, names = list(), list()\n",
    "        # Input sequence (t-n, ... t-1)\n",
    "        for i in range(window, 0, -1):\n",
    "            cols.append(data.shift(i))\n",
    "            names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "        # Current timestep (t=0)\n",
    "        cols.append(data)\n",
    "        names += [('%s(t)' % (col)) for col in data.columns]\n",
    "        # Target timestep (t=lag)\n",
    "        cols.append(data.shift(-lag))\n",
    "        names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "        # Put it all together\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # Drop rows with NaN values\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "        return agg\n",
    "    df_items2 = df_items[['id','store_id']]\n",
    "    df_train = df_sales.merge(df_items2,left_on='item_id',right_on='id')\n",
    "    df_train[['date','item_id','item_count','store_id']]\n",
    "\n",
    "    df_train = df_train.sort_values('date').groupby(['item_id', 'store_id', 'date'], as_index=False)\n",
    "    df_train = df_train.agg({'item_count':['mean']})\n",
    "    df_train.columns = ['item', 'store', 'date', 'sales']\n",
    "    window = 29\n",
    "    future_span = 30\n",
    "    series = series_to_supervised(df_train.drop('date', axis=1), window=window, lag=future_span)\n",
    "    # series.head()\n",
    "    last_item = 'item(t-%d)' % window\n",
    "    last_store = 'store(t-%d)' % window\n",
    "    series = series[(series['store(t)'] == series[last_store])]\n",
    "    series = series[(series['item(t)'] == series[last_item])]\n",
    "    \n",
    "    columns_to_drop = [('%s(t+%d)' % (col, future_span)) for col in ['item', 'store']]\n",
    "    for i in range(window, 0, -1):\n",
    "        columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\n",
    "    series.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    series.drop(['item(t)', 'store(t)'], axis=1, inplace=True)\n",
    "    \n",
    "    # Label\n",
    "    labels_col = 'sales(t+%d)' % future_span\n",
    "    labels = series[labels_col]\n",
    "    series = series.drop(labels_col, axis=1)\n",
    "\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.4, random_state=0)\n",
    "    \n",
    "    X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "    print('Train set shape', X_train_series.shape)\n",
    "    print('Validation set shape', X_valid_series.shape)\n",
    "\n",
    "    epochs = 500 \n",
    "    batch = 256\n",
    "    lr = 0.0003\n",
    "    adam = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    model_cnn = Sequential()\n",
    "    model_cnn.add(Conv1D(filters=64, kernel_size=8, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "    model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "    model_cnn.add(Flatten())\n",
    "    model_cnn.add(Dense(50, activation='relu'))\n",
    "    model_cnn.add(Dropout(0.2))\n",
    "    model_cnn.add(Dense(1))\n",
    "    model_cnn.compile(loss='mse', \n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                      metrics=[\"mse\"],)\n",
    "    model_cnn.summary()\n",
    "    \n",
    "    cnn_history = model_cnn.fit(X_train_series, Y_train, callbacks=[CustomCallback()],\n",
    "    validation_data=(X_valid_series, Y_valid), epochs=num_epoch, verbose=2)\n",
    "\n",
    "    cnn_train_pred = model_cnn.predict(X_train_series)\n",
    "    cnn_valid_pred = model_cnn.predict(X_valid_series)\n",
    "    print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\n",
    "    print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc7a2446-ce4a-47ee-bd70-44eafffb944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.25.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26dbc709-cbf0-4e77-ba6f-a9f8f04f33bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.8/site-packages (2.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b37bf6e-9001-4e87-ac94-38f75decc705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 07:11:07.049205: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-12T07:11:09Z INFO     --------------------------------------------------------------------------------------\n",
      "2023-12-12T07:11:09Z INFO     Input Parameters: {'lr': '0.01', 'num_epoch': '20', 'is_dist': False, 'num_workers': 2}\n",
      "2023-12-12T07:11:09Z INFO     --------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape (64002, 30, 1)\n",
      "Validation set shape (42668, 30, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 23, 64)            576       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 11, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 704)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                35250     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,877\n",
      "Trainable params: 35,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 07:11:11.050113: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 07:11:11.050161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-cuda-0): /proc/driver/nvidia/version does not exist\n",
      "2023-12-12 07:11:11.050628: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:15Z INFO     Epoch 1/20. mse=145.6462 - loss=145.6462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 5s - loss: 145.6462 - mse: 145.6462 - val_loss: 102.2179 - val_mse: 102.2179 - 5s/epoch - 2ms/step\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:19Z INFO     Epoch 2/20. mse=120.3830 - loss=120.3830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 120.3830 - mse: 120.3830 - val_loss: 87.4106 - val_mse: 87.4106 - 4s/epoch - 2ms/step\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:24Z INFO     Epoch 3/20. mse=116.0792 - loss=116.0792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 116.0792 - mse: 116.0792 - val_loss: 85.5221 - val_mse: 85.5221 - 4s/epoch - 2ms/step\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:28Z INFO     Epoch 4/20. mse=114.3711 - loss=114.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 114.3711 - mse: 114.3711 - val_loss: 83.3947 - val_mse: 83.3947 - 4s/epoch - 2ms/step\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:32Z INFO     Epoch 5/20. mse=106.2375 - loss=106.2375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 106.2375 - mse: 106.2375 - val_loss: 101.2649 - val_mse: 101.2649 - 4s/epoch - 2ms/step\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:36Z INFO     Epoch 6/20. mse=110.1770 - loss=110.1770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 110.1770 - mse: 110.1770 - val_loss: 85.2306 - val_mse: 85.2306 - 4s/epoch - 2ms/step\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:40Z INFO     Epoch 7/20. mse=109.0216 - loss=109.0216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 109.0216 - mse: 109.0216 - val_loss: 88.4187 - val_mse: 88.4187 - 4s/epoch - 2ms/step\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:44Z INFO     Epoch 8/20. mse=110.8104 - loss=110.8104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 110.8104 - mse: 110.8104 - val_loss: 82.6970 - val_mse: 82.6970 - 4s/epoch - 2ms/step\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:48Z INFO     Epoch 9/20. mse=108.4231 - loss=108.4231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 108.4231 - mse: 108.4231 - val_loss: 79.8336 - val_mse: 79.8336 - 4s/epoch - 2ms/step\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:53Z INFO     Epoch 10/20. mse=108.5988 - loss=108.5988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 108.5988 - mse: 108.5988 - val_loss: 81.2364 - val_mse: 81.2364 - 4s/epoch - 2ms/step\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:11:57Z INFO     Epoch 11/20. mse=104.4657 - loss=104.4657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 104.4657 - mse: 104.4657 - val_loss: 80.0327 - val_mse: 80.0327 - 4s/epoch - 2ms/step\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:01Z INFO     Epoch 12/20. mse=104.1132 - loss=104.1132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 104.1132 - mse: 104.1132 - val_loss: 90.7794 - val_mse: 90.7794 - 4s/epoch - 2ms/step\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:05Z INFO     Epoch 13/20. mse=106.1882 - loss=106.1882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 106.1882 - mse: 106.1882 - val_loss: 84.4228 - val_mse: 84.4228 - 4s/epoch - 2ms/step\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:09Z INFO     Epoch 14/20. mse=101.9315 - loss=101.9315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 101.9315 - mse: 101.9315 - val_loss: 85.4656 - val_mse: 85.4656 - 4s/epoch - 2ms/step\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:13Z INFO     Epoch 15/20. mse=104.2003 - loss=104.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 104.2003 - mse: 104.2003 - val_loss: 82.2685 - val_mse: 82.2685 - 4s/epoch - 2ms/step\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:18Z INFO     Epoch 16/20. mse=102.3049 - loss=102.3049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 102.3049 - mse: 102.3049 - val_loss: 79.6154 - val_mse: 79.6154 - 4s/epoch - 2ms/step\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:22Z INFO     Epoch 17/20. mse=100.0522 - loss=100.0522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 100.0522 - mse: 100.0522 - val_loss: 80.9192 - val_mse: 80.9192 - 4s/epoch - 2ms/step\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:26Z INFO     Epoch 18/20. mse=103.6357 - loss=103.6357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 103.6357 - mse: 103.6357 - val_loss: 78.0535 - val_mse: 78.0535 - 4s/epoch - 2ms/step\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:30Z INFO     Epoch 19/20. mse=102.0118 - loss=102.0118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 102.0118 - mse: 102.0118 - val_loss: 79.6638 - val_mse: 79.6638 - 4s/epoch - 2ms/step\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12T07:12:35Z INFO     Epoch 20/20. mse=100.1161 - loss=100.1161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001/2001 - 4s - loss: 100.1161 - mse: 100.1161 - val_loss: 78.1213 - val_mse: 78.1213 - 4s/epoch - 2ms/step\n",
      "2001/2001 [==============================] - 2s 845us/step\n",
      "1334/1334 [==============================] - 1s 809us/step\n",
      "Train rmse: 9.127654801748104\n",
      "Validation rmse: 8.838623783583412\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters for Local Training.\n",
    "parameters = {\n",
    "    \"lr\": \"0.01\",\n",
    "    \"num_epoch\": \"20\",\n",
    "    \"is_dist\": False,\n",
    "    \"num_workers\": 2\n",
    "}\n",
    "\n",
    "# Train Model locally in the Notebook.\n",
    "demandforecast(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea1dd26f-0728-40d4-8d1e-56ac3a73f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 8005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de4abab-bcb1-45c0-9866-0e5179aeece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing the zipfile module \n",
    "# from zipfile import ZipFile \n",
    "  \n",
    "# # loading the temp.zip and creating a zip object \n",
    "# with ZipFile(\"/home/jovyan/sales_train.csv.zip\", 'r') as zObject: \n",
    "  \n",
    "#     # Extracting all the members of the zip  \n",
    "#     # into a specific location. \n",
    "#     zObject.extractall( \n",
    "#         path=\"/home/jovyan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4e6e69-99d2-4755-9fae-fd0cc54a4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment sagar-kant/forecast-cmaes-hpt2 has been created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Katib Experiment forecast-cmaes-hpt2 link <a href=\"/_/katib/#/katib/hp_monitor/sagar-kant/forecast-cmaes-hpt2\" target=\"_blank\">here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kubeflow import katib\n",
    "parameters = {\n",
    "    \"lr\": katib.search.double(min=0.01, max=0.4),\n",
    "    \"num_epoch\": katib.search.int(min=10, max=500),\n",
    "    \"is_dist\": False,\n",
    "    \"num_workers\": 1\n",
    "}\n",
    "exp_name = \"forecast-cmaes-hpt2\"\n",
    "katib_client = katib.KatibClient()\n",
    "\n",
    "katib_client.tune(\n",
    "    name=exp_name,\n",
    "    objective=demandforecast,\n",
    "    parameters=parameters,\n",
    "    base_image='docker.io/amaksimov/python_data_science:latest',\n",
    "    algorithm_name=\"cmaes\",\n",
    "    objective_metric_name=\"mse\",\n",
    "    additional_metric_names=[\"loss\"],\n",
    "    max_trial_count=100,\n",
    "    parallel_trial_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a00fb6d-c2dc-4fb4-9003-034f29a8c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/kubeflow/katib.git#subdirectory=sdk/python/v1beta1\n",
    "# !pip install git+https://github.com/kubeflow/training-operator.git#subdirectory=sdk/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f7f4fa-0e91-4147-873e-c9a788bff6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katib Experiment is Succeeded: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = katib_client.is_experiment_succeeded(exp_name)\n",
    "print(f\"Katib Experiment is Succeeded: {status}\\n\")\n",
    "\n",
    "best_hps = katib_client.get_optimal_hyperparameters(exp_name)\n",
    "\n",
    "if best_hps != None:\n",
    "    print(\"Current Optimal Trial\\n\")\n",
    "    print(best_hps)\n",
    "    \n",
    "    for hp in best_hps.parameter_assignments:\n",
    "        if hp.name == \"lr\":\n",
    "            best_lr = hp.value\n",
    "        else:\n",
    "            best_num_epoch = hp.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0adfb43-6f7d-458c-a2c8-45e5d433994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow import katib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281b73a4-b0f8-44cf-9562-564e6c9fe899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kubeflow-katib\n",
      "  Obtaining dependency information for kubeflow-katib from https://files.pythonhosted.org/packages/19/c4/28ced11972001ef18b4b9776ffb71940b89d36229ab7408c16b388f65379/kubeflow_katib-0.16.0-py3-none-any.whl.metadata\n",
      "  Using cached kubeflow_katib-0.16.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.8/site-packages (from kubeflow-katib) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.8/site-packages (from kubeflow-katib) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.8/site-packages (from kubeflow-katib) (68.1.2)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.8/site-packages (from kubeflow-katib) (1.26.16)\n",
      "Collecting kubernetes>=23.6.0 (from kubeflow-katib)\n",
      "  Obtaining dependency information for kubernetes>=23.6.0 from https://files.pythonhosted.org/packages/f5/6a/1f69c2d8b1ff03f8d8e10d801f4ac3016ed4c1b00aa9795732c6ec900bba/kubernetes-28.1.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached kubernetes-28.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: grpcio>=1.41.1 in /opt/conda/lib/python3.8/site-packages (from kubeflow-katib) (1.57.0)\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.19.5 in /opt/conda/lib/python3.8/site-packages (from kubeflow-katib) (3.19.6)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (2.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (5.4.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (1.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (1.6.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.8/site-packages (from kubernetes>=23.6.0->kubeflow-katib) (3.2.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes>=23.6.0->kubeflow-katib) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes>=23.6.0->kubeflow-katib) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes>=23.6.0->kubeflow-katib) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->kubernetes>=23.6.0->kubeflow-katib) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->kubernetes>=23.6.0->kubeflow-katib) (3.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=23.6.0->kubeflow-katib) (0.5.0)\n",
      "Using cached kubeflow_katib-0.16.0-py3-none-any.whl (107 kB)\n",
      "Using cached kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: kubernetes, kubeflow-katib\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 12.0.1\n",
      "    Uninstalling kubernetes-12.0.1:\n",
      "      Successfully uninstalled kubernetes-12.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.6.3 requires kubernetes<13,>=8.0.0, but you have kubernetes 28.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed kubeflow-katib-0.16.0 kubernetes-28.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kubeflow-katib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb9a9b-3455-416f-a493-553d1705a05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
