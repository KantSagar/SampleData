{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca604e62-3d37-45a0-9aa6-f5533bed98ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T17:34:59.759743Z",
     "iopub.status.busy": "2023-11-23T17:34:59.759171Z",
     "iopub.status.idle": "2023-11-23T17:35:00.265279Z",
     "shell.execute_reply": "2023-11-23T17:35:00.264732Z",
     "shell.execute_reply.started": "2023-11-23T17:34:59.759723Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import kfp\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "\n",
    "class KFPClientManager:\n",
    "    \"\"\"\n",
    "    A class that creates `kfp.Client` instances with Dex authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_url: str,\n",
    "        dex_username: str,\n",
    "        dex_password: str,\n",
    "        dex_auth_type: str = \"local\",\n",
    "        skip_tls_verify: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the KfpClient\n",
    "\n",
    "        :param api_url: the Kubeflow Pipelines API URL\n",
    "        :param skip_tls_verify: if True, skip TLS verification\n",
    "        :param dex_username: the Dex username\n",
    "        :param dex_password: the Dex password\n",
    "        :param dex_auth_type: the auth type to use if Dex has multiple enabled, one of: ['ldap', 'local']\n",
    "        \"\"\"\n",
    "        self._api_url = api_url\n",
    "        self._skip_tls_verify = skip_tls_verify\n",
    "        self._dex_username = dex_username\n",
    "        self._dex_password = dex_password\n",
    "        self._dex_auth_type = dex_auth_type\n",
    "        self._client = None\n",
    "\n",
    "        # ensure `dex_default_auth_type` is valid\n",
    "        if self._dex_auth_type not in [\"ldap\", \"local\"]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid `dex_auth_type` '{self._dex_auth_type}', must be one of: ['ldap', 'local']\"\n",
    "            )\n",
    "\n",
    "    def _get_session_cookies(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the session cookies by authenticating against Dex\n",
    "        :return: a string of session cookies in the form \"key1=value1; key2=value2\"\n",
    "        \"\"\"\n",
    "\n",
    "        # use a persistent session (for cookies)\n",
    "        s = requests.Session()\n",
    "\n",
    "        # disable SSL verification, if requested\n",
    "        if self._skip_tls_verify:\n",
    "            s.verify = False\n",
    "            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "        # GET the api_url, which should redirect to Dex\n",
    "        resp = s.get(self._api_url, allow_redirects=True)\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for GET against: {self._api_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the endpoint is unsecured\n",
    "        if len(resp.history) == 0:\n",
    "            # no cookies are needed\n",
    "            return \"\"\n",
    "\n",
    "        # if we are at `/auth?=xxxx` path, we need to select an auth type\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/auth$\", url_obj.path):\n",
    "            url_obj = url_obj._replace(\n",
    "                path=re.sub(r\"/auth$\", f\"/auth/{self._dex_auth_type}\", url_obj.path)\n",
    "            )\n",
    "\n",
    "        # if we are at `/auth/xxxx/login` path, then we are at the login page\n",
    "        if re.search(r\"/auth/.*/login$\", url_obj.path):\n",
    "            dex_login_url = url_obj.geturl()\n",
    "        else:\n",
    "            # otherwise, we need to follow a redirect to the login page\n",
    "            resp = s.get(url_obj.geturl(), allow_redirects=True)\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {url_obj.geturl()}\"\n",
    "                )\n",
    "            dex_login_url = resp.url\n",
    "\n",
    "        # attempt Dex login\n",
    "        resp = s.post(\n",
    "            dex_login_url,\n",
    "            data={\"login\": self._dex_username, \"password\": self._dex_password},\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for POST against: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the login credentials were probably invalid\n",
    "        if len(resp.history) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Login credentials are probably invalid - \"\n",
    "                f\"No redirect after POST to: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        return \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "\n",
    "    def _create_kfp_client(self) -> kfp.Client:\n",
    "        try:\n",
    "            session_cookies = self._get_session_cookies()\n",
    "        except Exception as ex:\n",
    "            raise RuntimeError(f\"Failed to get Dex session cookies\") from ex\n",
    "\n",
    "        # monkey patch the kfp.Client to support disabling SSL verification\n",
    "        # kfp only added support in v2: https://github.com/kubeflow/pipelines/pull/7174\n",
    "        original_load_config = kfp.Client._load_config\n",
    "\n",
    "        def patched_load_config(client_self, *args, **kwargs):\n",
    "            config = original_load_config(client_self, *args, **kwargs)\n",
    "            config.verify_ssl = not self._skip_tls_verify\n",
    "            return config\n",
    "\n",
    "        patched_kfp_client = kfp.Client\n",
    "        patched_kfp_client._load_config = patched_load_config\n",
    "\n",
    "        return patched_kfp_client(\n",
    "            host=self._api_url,\n",
    "            cookies=session_cookies,\n",
    "        )\n",
    "\n",
    "    def create_kfp_client(self) -> kfp.Client:\n",
    "        \"\"\"Get a newly authenticated Kubeflow Pipelines client.\"\"\"\n",
    "        return self._create_kfp_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.19.*\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     ---------------------------------------- 0.0/895.7 kB ? eta -:--:--\n",
      "     ---- ---------------------------------- 92.2/895.7 kB 2.6 MB/s eta 0:00:01\n",
      "     ----- -------------------------------- 122.9/895.7 kB 1.4 MB/s eta 0:00:01\n",
      "     -------- ----------------------------- 194.6/895.7 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------- --------------------- 399.4/895.7 kB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 686.1/895.7 kB 3.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  890.9/895.7 kB 3.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 895.7/895.7 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.4\n",
      "    Uninstalling protobuf-4.24.4:\n",
      "      Successfully uninstalled protobuf-4.24.4\n",
      "Successfully installed protobuf-3.19.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ska641\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ska641\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ska641\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SKA641\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "feast 0.31.1 requires protobuf<5,>3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "grpcio-reflection 1.56.0 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ska641\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ska641\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ska641\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.19.* --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b436a8-90f3-4a7f-8613-cac4966adb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SKA641\\AppData\\Roaming\\Python\\Python310\\site-packages\\kfp\\client\\client.py:158: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiments': [{'created_at': datetime.datetime(2023, 10, 6, 7, 15, 58, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'Default',\n",
      "                  'experiment_id': 'ab01fc28-681a-410a-a84a-ea4665bcad0b',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 10, 17, 6, 31, 38, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'xg_boost',\n",
      "                  'experiment_id': '2bf812e7-836e-49a0-95d5-067d8e33958b',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 10, 17, 6, 31, 53, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'untitled',\n",
      "                  'experiment_id': '52b96571-5b21-4d2b-8fae-f6f0a0fb1317',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 10, 17, 7, 25, 47, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'xgboost',\n",
      "                  'experiment_id': '344bccdb-4fe1-414b-8389-258cc864bc4e',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 10, 17, 7, 35, 8, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'xgboost_pipeline',\n",
      "                  'experiment_id': '64ffb5d2-bb29-4f36-b496-3b7f0fbd6e81',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 10, 24, 11, 16, 9, tzinfo=tzutc()),\n",
      "                  'description': 'horovod',\n",
      "                  'display_name': 'horovod',\n",
      "                  'experiment_id': '0a9f0d04-78c9-4c6f-bcea-741b2a8ae37d',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 11, 27, 8, 7, 33, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'test-end',\n",
      "                  'experiment_id': '241ee675-a367-4157-9a62-1e0cefbc5d64',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 11, 27, 8, 7, 39, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'untitled2',\n",
      "                  'experiment_id': 'f1b42ba5-0158-4f60-9ede-c032322f8655',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 11, 27, 9, 51, 5, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'demo-end-end',\n",
      "                  'experiment_id': '5a0ba4cb-ac4d-445f-9cd6-f0ecb57e9a7c',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2023, 11, 27, 9, 53, 20, tzinfo=tzutc()),\n",
      "                  'description': None,\n",
      "                  'display_name': 'demo-1',\n",
      "                  'experiment_id': '114d92f3-270f-405c-a68b-b0f44d8916e1',\n",
      "                  'namespace': 'sagar-kant',\n",
      "                  'storage_state': 'AVAILABLE'}],\n",
      " 'next_page_token': 'eyJTb3J0QnlGaWVsZE5hbWUiOiJDcmVhdGVkQXRJblNlYyIsIlNvcnRCeUZpZWxkVmFsdWUiOjE3MDEwODEwMzQsIlNvcnRCeUZpZWxkUHJlZml4IjoiZXhwZXJpbWVudHMuIiwiS2V5RmllbGROYW1lIjoiVVVJRCIsIktleUZpZWxkVmFsdWUiOiJmNmUzNTEyOS00OWI5LTQyZjEtOTQ1Ni1hOWJhODNmYTA3YTQiLCJLZXlGaWVsZFByZWZpeCI6ImV4cGVyaW1lbnRzLiIsIklzRGVzYyI6ZmFsc2UsIk1vZGVsTmFtZSI6ImV4cGVyaW1lbnRzIiwiRmlsdGVyIjpudWxsfQ==',\n",
      " 'total_size': 26}\n"
     ]
    }
   ],
   "source": [
    "# initialize a KFPClientManager\n",
    "kfp_client_manager = KFPClientManager(\n",
    "    api_url=\"https://kbf-dev.maestro.maersk.com/pipeline\",\n",
    "    skip_tls_verify=True,\n",
    "\n",
    "    dex_username=\"sagar.kant@maersk.com\",\n",
    "    dex_password=\"contributor@124\",\n",
    "\n",
    "    dex_auth_type=\"local\",\n",
    ")\n",
    "\n",
    "# get a newly authenticated KFP client\n",
    "# TIP: long-lived sessions might need to get a new client when their session expires\n",
    "kfp_client = kfp_client_manager.create_kfp_client()\n",
    "\n",
    "# test the client by listing experiments\n",
    "experiments = kfp_client.list_experiments(namespace=\"sagar-kant\")\n",
    "print(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c55bcf-8abc-40f0-8623-460857a61aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T17:36:18.328516Z",
     "iopub.status.busy": "2023-11-23T17:36:18.327996Z",
     "iopub.status.idle": "2023-11-23T17:36:18.331776Z",
     "shell.execute_reply": "2023-11-23T17:36:18.331295Z",
     "shell.execute_reply.started": "2023-11-23T17:36:18.328472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kfp\n",
    "# from kfp import dsl\n",
    "\n",
    "# def hello_op():\n",
    "#     return dsl.ContainerOp(\n",
    "#         name='Hello Kubeflow',\n",
    "#         image='python:3.7',\n",
    "#         command=['python', '-c'],\n",
    "#         arguments=['print(\"Hello Kubeflow!\")'],\n",
    "#     )\n",
    "\n",
    "# @dsl.pipeline(\n",
    "#    name='Hello Kubeflow Pipeline',\n",
    "#    description='A simple intro pipeline'\n",
    "# )\n",
    "# def hello_pipeline():\n",
    "#     hello = hello_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfa7f6e-6d13-45a0-9914-21e11aa305e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T17:36:21.463623Z",
     "iopub.status.busy": "2023-11-23T17:36:21.463020Z",
     "iopub.status.idle": "2023-11-23T17:36:21.483101Z",
     "shell.execute_reply": "2023-11-23T17:36:21.482582Z",
     "shell.execute_reply.started": "2023-11-23T17:36:21.463600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/dsl/_container_op.py:1261: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline_func = hello_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://kbf-dev.maestro.maersk.com/pipeline/#/experiments/details/540efaf8-ea9e-48a0-ba36-f9d299d3fc7e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experimentSagar = kfp_client.create_experiment('my-experiment', namespace='sagar-kant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11eb5b00-d352-4173-ab22-157f4bf9c1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kfp\n",
    "# # Assuming you've already set up the KFPClientManager as before\n",
    "# # kfp_client = kfp_client_manager.create_kfp_client()\n",
    "\n",
    "# # Upload the pipeline\n",
    "# pipeline_package_path = './hello_pipeline.pipeline.tar.gz'\n",
    "# pipeline_name = \"Hello Kubeflow Pipeline\"\n",
    "# pipeline = kfp_client.upload_pipeline(pipeline_package_path, pipeline_name)\n",
    "\n",
    "# # Run the pipeline\n",
    "# experiment_name = 'Hello Kubeflow Experiment'\n",
    "# run_name = pipeline_name + ' Run'\n",
    "# experiment = kfp_client.create_experiment(experiment_name)\n",
    "# run_result = kfp_client.run_pipeline(experiment.id, run_name, pipeline_package_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f58985-2f90-44fa-bc47-6119c46fc56e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next_page_token': 'eyJTb3J0QnlGaWVsZE5hbWUiOiJDcmVhdGVkQXRJblNlYyIsIlNvcnRCeUZpZWxkVmFsdWUiOjE2OTkwMDI5ODYsIlNvcnRCeUZpZWxkUHJlZml4IjoicGlwZWxpbmVzLiIsIktleUZpZWxkTmFtZSI6IlVVSUQiLCJLZXlGaWVsZFZhbHVlIjoiMDc1NjMzNmItMjgzYi00NGY5LWJhNmEtNjNhYTg2OTY2ZDQwIiwiS2V5RmllbGRQcmVmaXgiOiJwaXBlbGluZXMuIiwiSXNEZXNjIjpmYWxzZSwiTW9kZWxOYW1lIjoicGlwZWxpbmVzIiwiRmlsdGVyIjpudWxsfQ==',\n",
       " 'pipelines': [{'created_at': datetime.datetime(2023, 10, 5, 11, 45, 32, tzinfo=tzutc()),\n",
       "                'description': '[source '\n",
       "                               'code](https://github.com/kubeflow/pipelines/tree/63ca91850a9f42a357f3417110a3011ddbf43290/samples/tutorials/Data%20passing%20in%20python%20components) '\n",
       "                               'Shows how to pass data between python '\n",
       "                               'components.',\n",
       "                'display_name': '[Tutorial] Data passing in python components',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '14c583d2-e634-45c6-a588-17e342a8098e'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 5, 11, 45, 33, tzinfo=tzutc()),\n",
       "                'description': '[source '\n",
       "                               'code](https://github.com/kubeflow/pipelines/tree/63ca91850a9f42a357f3417110a3011ddbf43290/samples/tutorials/DSL%20-%20Control%20structures) '\n",
       "                               'Shows how to use conditional execution and '\n",
       "                               'exit handlers. This pipeline will randomly '\n",
       "                               'fail to demonstrate that the exit handler gets '\n",
       "                               'executed even in case of failure.',\n",
       "                'display_name': '[Tutorial] DSL - Control structures',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': 'e06c2318-62d6-4c72-aa5e-0613cd92d78c'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 12, 9, 19, 51, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `untitled2.pipeline`.',\n",
       "                'display_name': 'kube-mlflow',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '798fb9c2-43b2-4981-8669-851271fc6c0b'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 12, 9, 20, 19, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `untitled2.pipeline`.',\n",
       "                'display_name': 'untitled2',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '18ecdb49-faa8-4681-91ec-34e7647c8e35'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 12, 9, 39, 38, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `exp-1.pipeline`.',\n",
       "                'display_name': 'exp-1',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '12b5b64c-b1c8-43a2-8139-de4b284e7cbc'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 17, 6, 31, 38, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `untitled.pipeline`.',\n",
       "                'display_name': 'xg_boost',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': 'a3141d0a-72fd-478a-9b84-1930841a944b'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 17, 6, 31, 53, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `untitled.pipeline`.',\n",
       "                'display_name': 'untitled',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '0ee52012-6aab-409e-9c73-ec04327f65cd'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 17, 7, 25, 47, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `untitled.pipeline`.',\n",
       "                'display_name': 'xgboost',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': 'fb39d596-2d8e-4983-ac59-ffffc4d77de5'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 17, 7, 35, 8, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `untitled1.pipeline`.',\n",
       "                'display_name': 'xgboost_pipeline',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '590cf517-618e-4dca-8d7d-7bb4d0357196'},\n",
       "               {'created_at': datetime.datetime(2023, 10, 27, 2, 49, 43, tzinfo=tzutc()),\n",
       "                'description': 'Created with Elyra 3.15.0 pipeline editor '\n",
       "                               'using `kube-mlflow_edit.pipeline`.',\n",
       "                'display_name': 'kube-mlflow_edit',\n",
       "                'error': None,\n",
       "                'namespace': None,\n",
       "                'pipeline_id': '1d33a791-83d7-49c9-a4c1-7db764de39c7'}],\n",
       " 'total_size': 32}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp_client.list_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e068bb-3c16-4d31-908a-00e819a8a541",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T18:00:56.109773Z",
     "iopub.status.busy": "2023-11-23T18:00:56.109180Z",
     "iopub.status.idle": "2023-11-23T18:00:56.278776Z",
     "shell.execute_reply": "2023-11-23T18:00:56.278187Z",
     "shell.execute_reply.started": "2023-11-23T18:00:56.109751Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://kbf-dev.maestro.maersk.com/pipeline/#/experiments/details/2463fe4d-e5fc-4d77-ac27-c8852cee5fc5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = kfp_client.create_experiment('Demand-Forecast-Experimentation', namespace='sagar-kant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2463fe4d-e5fc-4d77-ac27-c8852cee5fc5'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a3c7e8-c80f-4967-8345-d3e3bb121edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfp_client.list_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487000fd-b971-468b-b1f0-380de49e7e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://kbf-dev.maestro.maersk.com/pipeline/#/runs/details/5f2fbd84-6388-4cdb-bbc6-d1e722ccaff7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=5f2fbd84-6388-4cdb-bbc6-d1e722ccaff7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit a pipeline run from the pipeline package 'pipeline.yaml' to the experiment 'my-experiment'\n",
    "kfp_client.create_run_from_pipeline_package('C:/Users/SKA641/demo_DF_3.yaml', experiment_id=experiment.experiment_id)\n",
    "\n",
    "# Submit a pipeline run from the pipeline function 'pipeline_func' to the experiment 'my-experiment'\n",
    "# kfp_client.create_run_from_pipeline_func(pipeline_func, experiment_id=experiment.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://kbf-dev.maestro.maersk.com/pipeline/#/runs/details/f20baeaf-d689-45e0-b4b7-38e4688f2345\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from kfp import client\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "# https://github.com/KantSagar/SampleData/blob/ff59cdd8c3c3f7cc3ecad61ef1cb3be32a19b2db/train.csv\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "\n",
    "# 'numpy==1.3.0','pytz==2023.2'\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0'])\n",
    "def create_dataset(iris_dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "\n",
    "    # csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    # csv_url='https://github.com/KantSagar/SampleData/blob/63d7d1fcb344b5aab4eadd14ec2095fad334bb87/iris.csv'\n",
    "    csv_url='https://github.com/KantSagar/SampleData/blob/63d7d1fcb344b5aab4eadd14ec2095fad334bb87/iris.txt'\n",
    "    col_names = [\n",
    "        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    ]\n",
    "    df = pd.read_csv(csv_url, names=col_names)\n",
    "    print(df.head(1))\n",
    "\n",
    "    with open(iris_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0','scipy==1.7.3','joblib==1.1.0','threadpoolctl==3.1.0'])\n",
    "def normalize_dataset(\n",
    "    input_iris_dataset: Input[Dataset],\n",
    "    normalized_iris_dataset: Output[Dataset],\n",
    "    standard_scaler: bool,\n",
    "    min_max_scaler: bool,\n",
    "):\n",
    "    if standard_scaler is min_max_scaler:\n",
    "        raise ValueError(\n",
    "            'Exactly one of standard_scaler or min_max_scaler must be True.')\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    with open(input_iris_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    labels = df.pop('Labels')\n",
    "\n",
    "    # if standard_scaler:\n",
    "    #     # scaler = StandardScaler()\n",
    "    # if min_max_scaler:\n",
    "    #     # scaler = MinMaxScaler()\n",
    "\n",
    "    # df = pd.DataFrame(scaler.fit_transform(df))\n",
    "    df['Labels'] = labels\n",
    "    with open(normalized_iris_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0','scipy==1.7.3','joblib==1.1.0','threadpoolctl==3.1.0'])\n",
    "def train_model(\n",
    "    normalized_iris_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    n_neighbors: int,\n",
    "):\n",
    "    import pickle\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    with open(normalized_iris_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    y = df.pop('Labels')\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    with open(model.path, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='iris-training-pipeline')\n",
    "def my_pipeline(\n",
    "    standard_scaler: bool,\n",
    "    min_max_scaler: bool,\n",
    "    neighbors: int\n",
    "):\n",
    "    \n",
    "    create_dataset_task = create_dataset()\n",
    "\n",
    "    normalize_dataset_task = normalize_dataset(\n",
    "        input_iris_dataset=create_dataset_task.outputs['iris_dataset'],\n",
    "        standard_scaler=True,\n",
    "        min_max_scaler=False)\n",
    "    \n",
    "    train_task=train_model(\n",
    "        normalized_iris_dataset=normalize_dataset_task.outputs['normalized_iris_dataset'],\n",
    "        n_neighbors=neighbors)\n",
    "\n",
    "    # # with dsl.ParallelFor(neighbors) as n_neighbors:\n",
    "    # train_task =train_model(\n",
    "    #         normalized_iris_dataset=normalize_dataset_task\n",
    "    #         .outputs['normalized_iris_dataset'],\n",
    "    #         n_neighbors=neighbors)\n",
    "\n",
    "\n",
    "# endpoint = 'http://localhost:8080/'\n",
    "# kfp_client = client.Client(host=endpoint)\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    arguments={\n",
    "        'min_max_scaler': True,\n",
    "        'standard_scaler': False,\n",
    "        'neighbors': 3\n",
    "        # 'neighbors': [3, 6, 9]\n",
    "    },\n",
    "    namespace=\"sagar-kant\",\n",
    "    experiment_id=experiment.experiment_id\n",
    ")\n",
    "# url = f'{endpoint}/#/runs/details/{run.run_id}'\n",
    "# print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SKA641\\Downloads\\kubeflow_client.ipynb Cell 15\u001b[0m line \u001b[0;36m<cell line: 161>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m     train_task\u001b[39m=\u001b[39mtrain_model(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m         train_dataset\u001b[39m=\u001b[39mcreate_dataset_task\u001b[39m.\u001b[39moutputs[\u001b[39m'\u001b[39m\u001b[39mtrain_dataset\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m         test_dataset\u001b[39m=\u001b[39mcreate_dataset_task\u001b[39m.\u001b[39moutputs[\u001b[39m'\u001b[39m\u001b[39mtest_dataset\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39m# fullfil_center_dataset=create_dataset_task.outputs['fullfil_center_dataset'],\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m         \u001b[39m# meal_info_dataset=create_dataset_task.outputs['meal_info_dataset'],\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m         n_neighbors\u001b[39m=\u001b[39mneighbors)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m     \u001b[39m# # with dsl.ParallelFor(neighbors) as n_neighbors:\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m     \u001b[39m# train_task =train_model(\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m     \u001b[39m#         normalized_iris_dataset=normalize_dataset_task\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39m# endpoint = 'http://localhost:8080/'\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m# kfp_client = client.Client(host=endpoint)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m run \u001b[39m=\u001b[39m kfp_client\u001b[39m.\u001b[39mcreate_run_from_pipeline_func(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     my_pipeline,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     arguments\u001b[39m=\u001b[39m{\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mneighbors\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m         \u001b[39m# 'neighbors': [3, 6, 9]\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m     },\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m     namespace\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msagar-kant\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m     experiment_id\u001b[39m=\u001b[39mexperiment\u001b[39m.\u001b[39mexperiment_id\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SKA641/Downloads/kubeflow_client.ipynb#X20sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from kfp import client\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0'])\n",
    "# def create_dataset(iris_dataset: Output[Dataset]):\n",
    "def create_dataset(train_dataset: Output[Dataset],test_dataset: Output[Dataset],fullfil_center_dataset: Output[Dataset],meal_info_dataset: Output[Dataset],):\n",
    "    import pandas as pd\n",
    "    csv_url_train='https://raw.githubusercontent.com/KantSagar/SampleData/master/train.csv'\n",
    "    # csv_url_test='https://raw.githubusercontent.com/KantSagar/SampleData/master/test.csv'\n",
    "    # csv_url_fullfil_center='https://raw.githubusercontent.com/KantSagar/SampleData/master/fulfilment_center_info.csv'\n",
    "    # csv_url_meal_info='https://raw.githubusercontent.com/KantSagar/SampleData/master/meal_info.csv'\n",
    "    # csv_url_train='https://github.com/KantSagar/SampleData/blob/b04b3a61bf49b8f4d4597d314a500fce10ce254c/train.txt'\n",
    "    # csv_url_test='https://github.com/KantSagar/SampleData/blob/ad1b0e599c8c45533949880d063315fac49f83f1/test.txt'\n",
    "    # csv_url_fullfil_center='https://github.com/KantSagar/SampleData/blob/c5923bd3eb5d5cff9dea7c89300eaf34f8707cf2/fulfilment_center_info.txt'\n",
    "    # csv_url_meal_info='https://github.com/KantSagar/SampleData/blob/01b9af5470f66507af8a62aa408ca052cefeb481/meal_info.txt'\n",
    "    # col_names_train=['id','week','center_id','meal_id','checkout_price','base_price','emailer_for_promotion','homepage_featured','num_orders']\n",
    "    # df_train = pd.read_csv(csv_url_train,names=col_names_train)\n",
    "    # col_names_test=['id','week','center_id','meal_id','checkout_price','base_price','emailer_for_promotion','homepage_featured']\n",
    "    # df_test = pd.read_csv(csv_url_test,names=col_names_test)\n",
    "    # col_names_fci=['center_id','city_code','region_code','center_type','op_area']\n",
    "    # df_fullfil_center = pd.read_csv(csv_url_fullfil_center,names=col_names_fci)\n",
    "    # col_names_mi=['meal_id','category','cuisine']\n",
    "    # df_meal_info = pd.read_csv(csv_url_meal_info,names=col_names_mi)\n",
    "    # csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    # csv_url='https://github.com/KantSagar/SampleData/blob/63d7d1fcb344b5aab4eadd14ec2095fad334bb87/iris.csv'\n",
    "    # csv_url='https://github.com/KantSagar/SampleData/blob/63d7d1fcb344b5aab4eadd14ec2095fad334bb87/iris.txt'\n",
    "    # col_names_train = [\n",
    "    #     'id', 'week', 'center_id', 'meal_id', 'checkout_price','base_price','emailer_for_promotion','homepage_featured',\n",
    "    # ]\n",
    "    # col_names_test = [\n",
    "    #     'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    # ]\n",
    "    # col_names_fullfil_center = [\n",
    "    #     'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    # ]\n",
    "    # col_names_meal_info = [\n",
    "    #     'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    # ]\n",
    "    df_train = pd.read_csv(csv_url_train)\n",
    "    # df_test = pd.read_csv(test)\n",
    "    # df_fullfil_center = pd.read_csv(fullfil_center)\n",
    "    # df_meal_info = pd.read_csv(meal_info)\n",
    "\n",
    "    # with open(iris_dataset.path, 'w') as f:\n",
    "    #     df_train.to_csv(f)\n",
    "    # print(df_train.head(1))\n",
    "    # data = pd.merge(df_train, df_fullfil_center, on='center_id')\n",
    "    # all_data = pd.merge(data, df_meal_info, on='meal_id')\n",
    "    # print(all_data.head(1))\n",
    "    # with open(train_dataset.path, 'w') as f1:\n",
    "    #     all_data.to_csv(f1)\n",
    "    # with open(test_dataset.path, 'w') as f2:\n",
    "    #     df_test.to_csv(f2)\n",
    "    # with open(fullfil_center_dataset.path, 'w') as f3:\n",
    "    #     df_fullfil_center.to_csv(f3)\n",
    "    # with open(meal_info_dataset.path, 'w') as f4:\n",
    "    #     df_meal_info.to_csv(f4)\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0','scipy==1.7.3','joblib==1.1.0','threadpoolctl==3.1.0'])\n",
    "def train_model(\n",
    "    train_dataset: Input[Dataset],\n",
    "    test_dataset: Input[Dataset],\n",
    "    # fullfil_center_dataset: Input[Dataset],\n",
    "    # meal_info_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    n_neighbors: int,\n",
    "):\n",
    "    import pickle\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # from sklearn.tree import DecisionTreeRegressor\n",
    "    # from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_log_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "\n",
    "    with open(train_dataset.path) as f5:\n",
    "        train = pd.read_csv(f5)\n",
    "    with open(test_dataset.path) as f6:\n",
    "        test = pd.read_csv(f6)\n",
    "    # with open(fullfil_center_dataset.path) as f7:\n",
    "    #     fullfil_center = pd.read_csv(f7)\n",
    "    # with open(meal_info_dataset.path) as f8:\n",
    "    #     meal_info = pd.read_csv(f8)\n",
    "    \n",
    "    # print(train.head(1))\n",
    "    # print(test.head(1))\n",
    "    # print(fullfil_center_dataset.head(1))\n",
    "    # print(meal_info.head(1))\n",
    "    # data = pd.merge(train, fullfil_center, on='center_id')\n",
    "    # print(data.head(1))\n",
    "    # all_data = pd.merge(data, meal_info, on='meal_id')\n",
    "    # data_cp = all_data.copy()\n",
    "    data_cp=train\n",
    "    lb_enc = LabelEncoder()\n",
    "    data_cp[\"make_Cent_type\"] = lb_enc.fit_transform(data_cp[\"center_type\"])\n",
    "    data_cp[\"make_category\"] = lb_enc.fit_transform(data_cp[\"category\"])\n",
    "    data_cp[\"make_cuisine\"] = lb_enc.fit_transform(data_cp[\"cuisine\"])\n",
    "    cp_data = data_cp.drop(['center_type','category','cuisine'], axis=1)\n",
    "    X = cp_data.drop('num_orders', axis=1)\n",
    "    y = cp_data['num_orders']\n",
    "    print(X.head(1))\n",
    "    print(y.head(1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,train_size=0.8,random_state=0)\n",
    "    lin_reg_model = LinearRegression()\n",
    "    lin_reg_model.fit(X_train, y_train)\n",
    "    predict_train = lin_reg_model.predict(X_train)\n",
    "    rmse_train = mean_squared_error(y_train,predict_train)**(0.5)\n",
    "        \n",
    "    \n",
    "\n",
    "    # y = df.pop('Labels')\n",
    "    # X = df\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    # clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    # clf.fit(X_train, y_train)\n",
    "    with open(model.path, 'wb') as f:\n",
    "        pickle.dump(lin_reg_model, f)\n",
    "@dsl.pipeline(name='DF-training-pipeline')\n",
    "def my_pipeline(\n",
    "    neighbors: int\n",
    "):\n",
    "    \n",
    "    \n",
    "    create_dataset_task = create_dataset()\n",
    "\n",
    "    # normalize_dataset_task = normalize_dataset(\n",
    "    #     input_iris_dataset=create_dataset_task.outputs['iris_dataset'],\n",
    "    #     standard_scaler=True,\n",
    "    #     min_max_scaler=False)\n",
    "    \n",
    "    train_task=train_model(\n",
    "        train_dataset=create_dataset_task.outputs['train_dataset'],\n",
    "        test_dataset=create_dataset_task.outputs['test_dataset'],\n",
    "        # fullfil_center_dataset=create_dataset_task.outputs['fullfil_center_dataset'],\n",
    "        # meal_info_dataset=create_dataset_task.outputs['meal_info_dataset'],\n",
    "        n_neighbors=neighbors)\n",
    "\n",
    "    # # with dsl.ParallelFor(neighbors) as n_neighbors:\n",
    "    # train_task =train_model(\n",
    "    #         normalized_iris_dataset=normalize_dataset_task\n",
    "    #         .outputs['normalized_iris_dataset'],\n",
    "    #         n_neighbors=neighbors)\n",
    "\n",
    "\n",
    "# endpoint = 'http://localhost:8080/'\n",
    "# kfp_client = client.Client(host=endpoint)\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    arguments={\n",
    "        'neighbors': 3\n",
    "        # 'neighbors': [3, 6, 9]\n",
    "    },\n",
    "    namespace=\"sagar-kant\",\n",
    "    experiment_id=experiment.experiment_id\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://kbf-dev.maestro.maersk.com/pipeline/#/runs/details/f61e8f85-2dbb-4c2e-ac36-50a6e35edc47\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from kfp import client\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0'])\n",
    "# def create_dataset(iris_dataset: Output[Dataset]):\n",
    "def create_dataset(train_dataset: Output[Dataset],test_dataset: Output[Dataset],fullfil_center_dataset: Output[Dataset],meal_info_dataset: Output[Dataset],):\n",
    "    import pandas as pd\n",
    "    csv_url_train='https://raw.githubusercontent.com/KantSagar/SampleData/master/train.csv'\n",
    "    csv_url_test='https://raw.githubusercontent.com/KantSagar/SampleData/master/test.csv'\n",
    "    csv_url_fullfil_center='https://raw.githubusercontent.com/KantSagar/SampleData/master/fulfilment_center_info.csv'\n",
    "    csv_url_meal_info='https://raw.githubusercontent.com/KantSagar/SampleData/master/meal_info.csv'\n",
    "    # csv_url_train='https://github.com/KantSagar/SampleData/blob/b04b3a61bf49b8f4d4597d314a500fce10ce254c/train.txt'\n",
    "    # csv_url_test='https://github.com/KantSagar/SampleData/blob/ad1b0e599c8c45533949880d063315fac49f83f1/test.txt'\n",
    "    # csv_url_fullfil_center='https://github.com/KantSagar/SampleData/blob/c5923bd3eb5d5cff9dea7c89300eaf34f8707cf2/fulfilment_center_info.txt'\n",
    "    # csv_url_meal_info='https://github.com/KantSagar/SampleData/blob/01b9af5470f66507af8a62aa408ca052cefeb481/meal_info.txt'\n",
    "    # col_names_train=['id','week','center_id','meal_id','checkout_price','base_price','emailer_for_promotion','homepage_featured','num_orders']\n",
    "    # df_train = pd.read_csv(csv_url_train,names=col_names_train)\n",
    "    # col_names_test=['id','week','center_id','meal_id','checkout_price','base_price','emailer_for_promotion','homepage_featured']\n",
    "    # df_test = pd.read_csv(csv_url_test,names=col_names_test)\n",
    "    # col_names_fci=['center_id','city_code','region_code','center_type','op_area']\n",
    "    # df_fullfil_center = pd.read_csv(csv_url_fullfil_center,names=col_names_fci)\n",
    "    # col_names_mi=['meal_id','category','cuisine']\n",
    "    # df_meal_info = pd.read_csv(csv_url_meal_info,names=col_names_mi)\n",
    "    # csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    # csv_url='https://github.com/KantSagar/SampleData/blob/63d7d1fcb344b5aab4eadd14ec2095fad334bb87/iris.csv'\n",
    "    # csv_url='https://github.com/KantSagar/SampleData/blob/63d7d1fcb344b5aab4eadd14ec2095fad334bb87/iris.txt'\n",
    "    # col_names_train = [\n",
    "    #     'id', 'week', 'center_id', 'meal_id', 'checkout_price','base_price','emailer_for_promotion','homepage_featured',\n",
    "    # ]\n",
    "    # col_names_test = [\n",
    "    #     'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    # ]\n",
    "    # col_names_fullfil_center = [\n",
    "    #     'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    # ]\n",
    "    # col_names_meal_info = [\n",
    "    #     'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    # ]\n",
    "    df_train = pd.read_csv(csv_url_train)\n",
    "    df_test = pd.read_csv(csv_url_test)\n",
    "    df_fullfil_center = pd.read_csv(csv_url_fullfil_center)\n",
    "    df_meal_info = pd.read_csv(csv_url_meal_info)\n",
    "\n",
    "    # with open(iris_dataset.path, 'w') as f:\n",
    "    #     df_train.to_csv(f)\n",
    "    print(df_train.head(1))\n",
    "    print(df_test.head(1))\n",
    "    print(df_fullfil_center.head(1))\n",
    "    print(df_meal_info.head(1))\n",
    "    # data = pd.merge(df_train, df_fullfil_center, on='center_id')\n",
    "    # all_data = pd.merge(data, df_meal_info, on='meal_id')\n",
    "    # print(all_data.head(1))\n",
    "    with open(train_dataset.path, 'w') as f1:\n",
    "        df_train.to_csv(f1)\n",
    "    with open(test_dataset.path, 'w') as f2:\n",
    "        df_test.to_csv(f2)\n",
    "    with open(fullfil_center_dataset.path, 'w') as f3:\n",
    "        df_fullfil_center.to_csv(f3)\n",
    "    with open(meal_info_dataset.path, 'w') as f4:\n",
    "        df_meal_info.to_csv(f4)\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2','numpy==1.21.6','pytz==2022.1','python-dateutil==2.8.2','six==1.16.0','scipy==1.7.3','joblib==1.1.0','threadpoolctl==3.1.0'])\n",
    "def train_model(\n",
    "    train_dataset: Input[Dataset],\n",
    "    test_dataset: Input[Dataset],\n",
    "    fullfil_center_dataset: Input[Dataset],\n",
    "    meal_info_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    n_neighbors: int,\n",
    "):\n",
    "    import pickle\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # from sklearn.tree import DecisionTreeRegressor\n",
    "    # from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_log_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "\n",
    "    with open(train_dataset.path) as f5:\n",
    "        train = pd.read_csv(f5)\n",
    "    with open(test_dataset.path) as f6:\n",
    "        test = pd.read_csv(f6)\n",
    "    with open(fullfil_center_dataset.path) as f7:\n",
    "        fullfil_center = pd.read_csv(f7)\n",
    "    with open(meal_info_dataset.path) as f8:\n",
    "        meal_info = pd.read_csv(f8)\n",
    "    \n",
    "    # print(train.head(1))\n",
    "    # print(test.head(1))\n",
    "    # print(fullfil_center_dataset.head(1))\n",
    "    # print(meal_info.head(1))\n",
    "    data = pd.merge(train, fullfil_center, on='center_id')\n",
    "    print(data.head(1))\n",
    "    all_data = pd.merge(data, meal_info, on='meal_id')\n",
    "    data_cp = all_data.copy()\n",
    "    # data_cp=train\n",
    "    lb_enc = LabelEncoder()\n",
    "    data_cp[\"make_Cent_type\"] = lb_enc.fit_transform(data_cp[\"center_type\"])\n",
    "    data_cp[\"make_category\"] = lb_enc.fit_transform(data_cp[\"category\"])\n",
    "    data_cp[\"make_cuisine\"] = lb_enc.fit_transform(data_cp[\"cuisine\"])\n",
    "    cp_data = data_cp.drop(['center_type','category','cuisine'], axis=1)\n",
    "    X = cp_data.drop('num_orders', axis=1)\n",
    "    y = cp_data['num_orders']\n",
    "    print(X.head(1))\n",
    "    print(y.head(1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,train_size=0.8,random_state=0)\n",
    "    lin_reg_model = LinearRegression()\n",
    "    lin_reg_model.fit(X_train, y_train)\n",
    "    predict_train = lin_reg_model.predict(X_train)\n",
    "    rmse_train = mean_squared_error(y_train,predict_train)**(0.5)\n",
    "        \n",
    "    \n",
    "\n",
    "    # y = df.pop('Labels')\n",
    "    # X = df\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    # clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    # clf.fit(X_train, y_train)\n",
    "    with open(model.path, 'wb') as f:\n",
    "        pickle.dump(lin_reg_model, f)\n",
    "@dsl.pipeline(name='Demand-Forcasting-MW')\n",
    "def my_pipeline(\n",
    "    neighbors: int\n",
    "):\n",
    "    \n",
    "    \n",
    "    create_dataset_task = create_dataset()\n",
    "\n",
    "    # normalize_dataset_task = normalize_dataset(\n",
    "    #     input_iris_dataset=create_dataset_task.outputs['iris_dataset'],\n",
    "    #     standard_scaler=True,\n",
    "    #     min_max_scaler=False)\n",
    "    \n",
    "    train_task=train_model(\n",
    "        train_dataset=create_dataset_task.outputs['train_dataset'],\n",
    "        test_dataset=create_dataset_task.outputs['test_dataset'],\n",
    "        fullfil_center_dataset=create_dataset_task.outputs['fullfil_center_dataset'],\n",
    "        meal_info_dataset=create_dataset_task.outputs['meal_info_dataset'],\n",
    "        n_neighbors=neighbors)\n",
    "\n",
    "    # # with dsl.ParallelFor(neighbors) as n_neighbors:\n",
    "    # train_task =train_model(\n",
    "    #         normalized_iris_dataset=normalize_dataset_task\n",
    "    #         .outputs['normalized_iris_dataset'],\n",
    "    #         n_neighbors=neighbors)\n",
    "\n",
    "\n",
    "# endpoint = 'http://localhost:8080/'\n",
    "# kfp_client = client.Client(host=endpoint)\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    arguments={\n",
    "        'neighbors': 3\n",
    "        # 'neighbors': [3, 6, 9]\n",
    "    },\n",
    "    namespace=\"sagar-kant\",\n",
    "    experiment_id=experiment.experiment_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
